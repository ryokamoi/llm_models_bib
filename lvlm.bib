comment = {Proprietary Models}

comment = {OpenAI}

comment = {GPT-4o}
@misc{gpt4o,
    title={Hello GPT-4o},
    author={{OpenAI}},
    year={2024},
    url={https://openai.com/index/hello-gpt-4o/}
}

comment = {GPT-4o-mini}
@misc{gpt4o-mini,
    title={GPT-4o mini: advancing cost-efficient intelligence},
    author={{OpenAI}},
    year={2024},
    url={https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/}
}

comment = {GPT-4}
@article{gpt4,
  title={GPT-4 Technical Report},
  author={{OpenAI}},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

comment = {Anthropic}

comment = {Claude 3.5}
@misc{claude35,
    title={Claude 3.5 Sonnet Model Card Addendum},
    author={{Anthropic}},
    year={2024},
    url={https://www-cdn.anthropic.com/fed9cc193a14b84131812372d8d5857f8f304c52/Model_Card_Claude_3_Addendum.pdf}
}

comment = {Google}

comment = {Gemini 1.5}
@misc{gemini15,
    title={Our next-generation model: Gemini 1.5},
    author={{Google}},
    year={2024},
    url={https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/}
}


comment = {Open Models}

comment = {LLaVA}

comment = {LLaVA-NeXT-Interleave}
@article{li2024llava-next-interleave,
    title={LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models},
    author={Li, Feng and Zhang, Renrui and Zhang, Hao and Zhang, Yuanhan and Li, Bo and Li, Wei and Ma, Zejun and Li, Chunyuan},
    journal={arXiv preprint arXiv:2407.07895},
    year={2024}
}

comment = {LLaVA-1.5}
@InProceedings{llava15,
    author    = {Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
    title     = {Improved Baselines with Visual Instruction Tuning},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {26296-26306}
}

comment = {LLaVA-1.0}
@inproceedings{llava10,
    title={Visual Instruction Tuning},
    author={Haotian Liu and Chunyuan Li and Qingyang Wu and Yong Jae Lee},
    booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
    year={2023},
    url={https://openreview.net/forum?id=w0H2xGHlkw}
}

comment = {InternVL}

comment = {InternVL2.0}
@misc{internvl20,
    title={InternVL2: Better than the Best—Expanding Performance Boundaries of Open-Source Multimodal Models with the Progressive Scaling Strategy},
    author={{OpenGVLab Team}},
    year={2024},
    url={https://internvl.github.io/blog/2024-07-02-InternVL-2.0/}
}

comment = {InternVL1.5}
@article{internvl15,
  title={How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites},
  author={Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others},
  journal={arXiv preprint arXiv:2404.16821},
  year={2024}
}

comment = {internvl1.0}
@article{internvl10,
  title={InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks},
  author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and Li, Bin and Luo, Ping and Lu, Tong and Qiao, Yu and Dai, Jifeng},
  journal={arXiv preprint arXiv:2312.14238},
  year={2023}
}

comment = {Microsoft}

comment = {Phi-3.5}
@misc{phi-35,
    title={Discover the New Multi-Lingual, High-Quality Phi-3.5 SLMs},
    author={{Microsoft}},
    year={2024},
    url={https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/discover-the-new-multi-lingual-high-quality-phi-3-5-slms/ba-p/4225280}
}

comment = {Phi-3}
@article{abdin2024phi3technicalreporthighly,
      title={Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone}, 
      author={Marah Abdin and Jyoti Aneja and Hany Awadalla and Ahmed Awadallah and Ammar Ahmad Awan and Nguyen Bach and Amit Bahree and Arash Bakhtiari and Jianmin Bao and Harkirat Behl and Alon Benhaim and Misha Bilenko and Johan Bjorck and Sébastien Bubeck and Martin Cai and Qin Cai and Vishrav Chaudhary and Dong Chen and Dongdong Chen and Weizhu Chen and Yen-Chun Chen and Yi-Ling Chen and Hao Cheng and Parul Chopra and Xiyang Dai and Matthew Dixon and Ronen Eldan and Victor Fragoso and Jianfeng Gao and Mei Gao and Min Gao and Amit Garg and Allie Del Giorno and Abhishek Goswami and Suriya Gunasekar and Emman Haider and Junheng Hao and Russell J. Hewett and Wenxiang Hu and Jamie Huynh and Dan Iter and Sam Ade Jacobs and Mojan Javaheripi and Xin Jin and Nikos Karampatziakis and Piero Kauffmann and Mahoud Khademi and Dongwoo Kim and Young Jin Kim and Lev Kurilenko and James R. Lee and Yin Tat Lee and Yuanzhi Li and Yunsheng Li and Chen Liang and Lars Liden and Xihui Lin and Zeqi Lin and Ce Liu and Liyuan Liu and Mengchen Liu and Weishung Liu and Xiaodong Liu and Chong Luo and Piyush Madan and Ali Mahmoudzadeh and David Majercak and Matt Mazzola and Caio César Teodoro Mendes and Arindam Mitra and Hardik Modi and Anh Nguyen and Brandon Norick and Barun Patra and Daniel Perez-Becker and Thomas Portet and Reid Pryzant and Heyang Qin and Marko Radmilac and Liliang Ren and Gustavo de Rosa and Corby Rosset and Sambudha Roy and Olatunji Ruwase and Olli Saarikivi and Amin Saied and Adil Salim and Michael Santacroce and Shital Shah and Ning Shang and Hiteshi Sharma and Yelong Shen and Swadheen Shukla and Xia Song and Masahiro Tanaka and Andrea Tupini and Praneetha Vaddamanu and Chunyu Wang and Guanhua Wang and Lijuan Wang and Shuohang Wang and Xin Wang and Yu Wang and Rachel Ward and Wen Wen and Philipp Witte and Haiping Wu and Xiaoxia Wu and Michael Wyatt and Bin Xiao and Can Xu and Jiahang Xu and Weijian Xu and Jilong Xue and Sonali Yadav and Fan Yang and Jianwei Yang and Yifan Yang and Ziyi Yang and Donghan Yu and Lu Yuan and Chenruidong Zhang and Cyril Zhang and Jianwen Zhang and Li Lyna Zhang and Yi Zhang and Yue Zhang and Yunan Zhang and Xiren Zhou},
      year={2024},
      journal={arXiv preprint arXiv:2404.14219},
}
